#!/usr/bin/env python3
#TODO CHANGE this file to load each model and continue training with Adam for several epochs with declining LR
import time
import sys
import os
import numpy as np
import re 
sys.path.append("../../code/utils")
import utils
import os
from keras.models import Model,load_model
import keras.optimizers 
from keras import backend as K

modelsDir = 'input/'
outputdir = 'output/'
#models = ["TB_FTMC_Xception_v71"]
dataDirPrefix = "../../B2split_"
#eras = [0,1] 
crossValSet = range(5) 
batch_size=32
steps_per_epoch=36 #determine with count
validation_steps=9
inputWidth = 299
inputHeight = 299
target_size=(inputWidth, inputHeight) 
# Data Augmentation code 
imgDatagen = im.ImageDataGenerator(
        #shear_range=0.1, 1.3
        zoom_range=[0.9,1],
        horizontal_flip=True,
        vertical_flip=True,
        width_shift_range=0.2,
        height_shift_range=0.2,
        rotation_range=360,
        channel_shift_range=10,
        preprocessing_function=preprocess_input)
checkpointEnd="_ep{epoch:02d}-vl{val_loss:.2f}.hdf5"

for cvIndex in crossValSet: 
    for modelFN in os.listdir(modelsDir):
        if 'split'+cvindex in modelFN:
            model = load_model(modelsDir+modelFN)
            print(modelFN)
            lrvar =model.optimizer.lr
            print(str(lrvar) + ' == ' + str(K.eval(lrvar)))
            print("########## TRAINING model:" + str(modelFN) + ", cvindex" + str(cvIndex))
            newName = modelFN.split('_Era')[0] + eraName
            checkpointName='/tmp/'+newName
            savename= outputdir+newName+"_ep{epoch:02d}.hdf5"
            trainDir =dataDirPrefix+str(cvIndex)+'_Train'
            valDir = dataDirPrefix+str(cvIndex)+'_Val'
            #Make data generators
            trainDatagen = imgDatagen.flow_from_directory(
                        trainDir,
                        target_size=target_size,
                        batch_size=batch_size,
                        class_mode=class_mode,
                        follow_links=True)

            valDatagen = imgDatagen.flow_from_directory(
                    valDir,
                    target_size=target_size,
                    batch_size=batch_size,
                    class_mode=class_mode,
                    follow_links=True)

            #Start training this era
            nbOfEpochs=100
            eraName="_Era3"
    ##CALLBACKS
    #checkpoint callback
            checkpointer = ModelCheckpoint(filepath=checkpointName+checkpointEnd, verbose=1, save_best_only=True)
    # at this point, the top layers are well trained and we can start fine-tuning
    # convolutional layers from inception V3. We will freeze the bottom N layers
    # and train the remaining top layers.

            for i, layer in enumerate(self.baseModel.layers):
                print(i, layer.name)

    # we chose to train the top 2 inception blocks, i.e. we will freeze
    # the first 249 layers and unfreeze the rest:
            for layer in self.model.layers[:115]:
                layer.trainable = False
            for layer in self.model.layers[116:]:
                layer.trainable = True

    # we need to recompile the model for these modifications to take effect
    # we use Adam with a low learning rate
            #self.model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy')
            #MODEL SHOULD ALREADY BE COMPILED WITH THE OPTIMIZER STATE IN THE SAVEFILE

    # we train our model again (this time fine-tuning the top 2 inception blocks
    # alongside the top Dense layers
    # Same fit_generator?
            self.model.fit_generator(
                    trainGen,
                    steps_per_epoch=self.steps_per_epoch,
                    epochs=15, 
                    validation_data=valGen,
                    validation_steps=self.validation_steps,
                    callbacks=[checkpointer,self.early_stopping])

