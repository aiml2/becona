{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Becona problem specification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Becona is a concrete casting company. They rent out equipment and expertise to construction companies. When the equipment returns from rental it has to be cleaned, sorted and counted. Currently, the last two tasks are done by (heavy) manual labour. A more automated sorting system could reduce the amount of manual labour/concentration needed and hopefully increase accuracy of the sorting. The throughput of such a machine would be ideally 1 item/second. The assembly line sorting machinery can be made in house. The camera(s) and computation hardware can be bought (sub 1000 euro for both is rough estimate).\n",
    "\n",
    "The classification problem for sorting the equipment is the main focus of this project.\n",
    "\n",
    "## Equipment\n",
    "\n",
    "There are numerous (~30) pieces of equipment and they range broadly in size: Ranging from the size of a pencil to the size of a shovel. Several pieces of the equipment have movable parts. Some pieces of equipment have different makes with possible differences between batches.\n",
    "\n",
    "Each piece is designed in-house and 3D CAD files are available for each of them. Different batches of the same equipment can have slight physical changes: other materials, reworked form but. On top of that comes the wear and tear of the equipment: damaged coating, leftover dirt, concrete smudges, chipped of parts that don't impede the functionality.\n",
    "\n",
    "## Equipment Classification\n",
    "\n",
    "For sorting \n",
    "\n",
    "\n",
    "# Current Progress\n",
    "\n",
    "Focus of this initial research phase is to get a feel for the problem and prototype some models to see how viable it is. As well as a kind of playground for me to get some appied ML experience. \n",
    "\n",
    "## Data Collection\n",
    "\n",
    "\n",
    "\n",
    "1500 hand taken photos spanning six different classes. Attempted to capture as much variation as possible in a non-orderly way. Resulting in photos whith different illumination, wear and tear, make, background and point of view. \n",
    "* 1 Spanklem\n",
    "* 2.0 VleugelmoerOplegRecht_Oud_rond_twee_gaten\n",
    "* 2.1 VleugelMoerOplegRecht_Nieuw_geen_gaten\n",
    "* 3_VleugelMoerOpleg_Rond\n",
    "* 4.0_Variable_Spanklem_Kort\n",
    "* 4.1_Variable_Spanklem_Lang\n",
    "\n",
    "\n",
    "2.0 and 2.1 differ in make but are functionally identical\n",
    "4.0 and 4.1 differ in sizes.\n",
    "\n",
    "Hand picked some similar looking (and very commonly used) pieces of equipment.\n",
    "2.0, 2.1 and 3 look alike\n",
    "1, 4.0 and 4.1 also look alike\n",
    "\n",
    "### Cross-Validation sets\n",
    "\n",
    "Currently a 5-fold cross validation is done for each of the different models described below.\n",
    "For each model 20% of the data is held out and not trained upon but soley used as validation set.\n",
    "\n",
    "Currently no test set is used.\n",
    "\n",
    "## Data Augmentation\n",
    "\n",
    "Since I have a quite small dataset (and to prevent overfitting) I employ data augmentations:\n",
    "* zooming [0.8-1]\n",
    "* horizontal flips\n",
    "* vertical flips\n",
    "* width shift [0,0.2]\n",
    "* height shift [0,0.2]\n",
    "* rotations [0-360]\n",
    "* channel shift [0-10]\n",
    "\n",
    "## Transfer learning\n",
    "\n",
    "Currently I'm using Keras with the tensorflow backend to do transfer learning from pretrained imagenet models.\n",
    "\n",
    "I have 6 different models trained so far on the data I gathered right now. Always following this two-Era strategy for training:\n",
    "\n",
    "0) Training Era 0: Train initially with base network as feature extractor using SGD.\n",
    "\n",
    "1) Training Era 1: Train with top X layers of the base network trainable using SGD and simple time-based learning rate decay.\n",
    "\n",
    "\n",
    "### From Inception_V3\n",
    "\n",
    "#### Layers of InceptionV3_v3\n",
    "\n",
    "- InceptionV3\n",
    "- GlobalAveragePooling2D\n",
    "- Dense(256, activation='relu')\n",
    "- Dense(nbOfClasses, activation='softmax')\n",
    "\n",
    "#### Layers of InceptionV3_v4\n",
    "\n",
    "- InceptionV3\n",
    "- GlobalAveragePooling2D\n",
    "- Dense(256, activation='relu')\n",
    "- Dropout(0.5)\n",
    "- Dense(nbOfClasses, activation='softmax')\n",
    "\n",
    "\n",
    "### From Xception\n",
    "\n",
    "#### Layers of Xception_v3\n",
    "- Xception\n",
    "- GlobalAveragePooling2D\n",
    "- Dense(256, activation='relu')\n",
    "- Dense(nbOfClasses, activation='softmax')\n",
    "\n",
    "#### Layers of Xception_v4\n",
    "\n",
    "- Xception\n",
    "- GlobalAveragePooling2D\n",
    "- Dense(256, activation='relu')\n",
    "- Dropout(0.5)\n",
    "- Dense(nbOfClasses, activation='softmax')\n",
    "\n",
    "#### Layers of Xception_v5\n",
    "- Xception\n",
    "- GlobalAveragePooling2D\n",
    "- Dense(128, activation='relu')\n",
    "- Dense(nbOfClasses, activation='softmax')\n",
    "\n",
    "#### Layers of Xception_v6\n",
    "\n",
    "- Xception\n",
    "- GlobalAveragePooling2D\n",
    "- Dense(128, activation='relu')\n",
    "- Dropout(0.5)\n",
    "- Dense(nbOfClasses, activation='softmax')\n",
    "\n",
    "# Problems \n",
    "\n",
    "I currently have >186 trained models. For each of the different model configs trained on each of the five cross validation sets and for each of those I save the weights of the last epoch of Era 0 and all the weights of epochs that were improvements of Era 1.\n",
    "\n",
    "# Ideas \n",
    "\n",
    "\n",
    "\n",
    "### Using the CAD file data\n",
    "\n",
    "Mapping image to class + view / rotation parameters on the CAD file.\n",
    "\n",
    "\n",
    "### Using RGBD data\n",
    "\n",
    "Microsoft Kinect sensor?\n",
    "\n",
    "\n",
    "# Additional Classifications\n",
    "\n",
    "Sometimes functionally equal equipment from competitors is returned instead of the Becona equipment. While sorting the non-becona items should be placed in a seperate bin.\n",
    "\n",
    "Being able to distinguish between the makes of the same equipment could be a nice addition for the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['prediction_testing_HQ/IMG_20170904_142543.jpg' '0.8159372' '1.5MiB']\n",
      " ['prediction_testing_HQ/IMG_20170904_142553.jpg' '0.5424644' '2.3MiB']\n",
      " ['prediction_testing_HQ/IMG_20170904_142701.jpg' '0.9022982' '2.4MiB']\n",
      " ['prediction_testing_HQ/IMG_20170904_142710.jpg' '0.5279898' '2.4MiB']\n",
      " ['prediction_testing_HQ/IMG_20170904_145514.jpg' '0.938184' '1.5MiB']\n",
      " ['prediction_testing_HQ/IMG_20170904_145521.jpg' '0.7246802' '2.1MiB']\n",
      " ['prediction_testing_HQ/IMG_20170904_145528.jpg' '0.9941446' '1.6MiB']\n",
      " ['prediction_testing_HQ/IMG_20170904_145533.jpg' '0.9001809' '1.1MiB']\n",
      " ['prediction_testing_HQ/IMG_20170904_145547.jpg' '0.94790196' '1.7MiB']\n",
      " ['prediction_testing_HQ/IMG_20170904_163433.jpg' '0.58329237' '1.5MiB']\n",
      " ['prediction_testing_HQ/IMG_20170904_163444.jpg' '0.8570568' '1.2MiB']\n",
      " ['prediction_testing_HQ/IMG_20170904_163542.jpg' '0.999666' '1.5MiB']\n",
      " ['prediction_testing_HQ/IMG_20170904_163551.jpg' '0.8710418' '1.4MiB']\n",
      " ['prediction_testing_HQ/IMG_20170904_163557.jpg' '0.9969921' '1.5MiB']\n",
      " ['prediction_testing_HQ/IMG_20170904_163600.jpg' '0.6915053' '1.3MiB']\n",
      " ['prediction_testing_HQ/IMG_20170904_163651.jpg' '0.89588296' '2.7MiB']\n",
      " ['prediction_testing_HQ/NewRedBot.jpg' '0.5768584' '1.4MiB']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sys.path.append(\"code/utils\")\n",
    "import utils\n",
    "data = np.load('lastresults.npz')\n",
    "print(data['wrongsWithConfid'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"code/utils\")\n",
    "import utils"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
